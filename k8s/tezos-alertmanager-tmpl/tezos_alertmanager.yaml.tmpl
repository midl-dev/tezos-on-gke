apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus-${kubernetes_namespace}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    app: prometheus-xtz
    release: mon-xtz
  name: prometheus-xtz
spec:
  serviceAccountName: prometheus
  alerting:
    alertmanagers:
    - apiVersion: v2
      name: alertmanager-xtz
      namespace: ${kubernetes_namespace}
      pathPrefix: /
      port: web
  baseImage: quay.io/prometheus/prometheus
  enableAdminAPI: false
  externalUrl: http://prometheus-xtz.mon:9090
  listenLocal: false
  logFormat: logfmt
  logLevel: info
  paused: false
  portName: web
  replicas: 1
  retention: 10d
  routePrefix: /
  ruleSelector:
    matchLabels:
      release: mon-xtz
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceMonitorSelector:
    matchLabels:
      release: mon-xtz
  version: v2.18.2
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: alertmanager-xtz
  labels:
    app: prometheus-xtz
    release: mon-xtz
spec:
  replicas: 1
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-xtz
spec:
  clusterIP: None
  ports:
  - name: web
    port: 9093
    protocol: TCP
    targetPort: web
  selector:
    alertmanager: alertmanager-xtz
---
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-alertmanager-xtz
  labels:
    app: prometheus-xtz
    release: mon-xtz
stringData:
  alertmanager.yaml: |-
    global:
      slack_api_url: "${monitoring_slack_url}"
      smtp_smarthost: '${monitoring_smtp_server}'
      smtp_auth_username: '${monitoring_smtp_username}'
      smtp_auth_password: '${monitoring_smtp_password}'
      smtp_require_tls: true
      smtp_from: '${monitoring_email_from}'
      resolve_timeout: 5m
    route:
      receiver: 'null'
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      routes:
      - match:
          alertname: KubePersistentVolumeFillingUp
        receiver: 'slack_master'
      - match:
          alertname: PrivateNodeLost1Connection
        receiver: 'slack_master'
      - match:
          alertname: PrivateNodeDisconnected
        receiver: 'slack_master'
%{ for baking_node in baking_nodes }
%{     for baker in keys(baking_node) }
%{ if contains(keys(baking_node[baker]), "monitoring_slack_url") }
      - match:
          alertname: NoRemoteSigner
          service: ${kubernetes_name_prefix}-tezos-remote-signer-loadbalancer-${baker}
        receiver: 'slack_${baker}'
        continue: true
      - match:
          alertname: SignerPowerLoss
          service: ${kubernetes_name_prefix}-tezos-remote-signer-forwarder-${baker}
        receiver: 'slack_${baker}'
        continue: true
      - match:
          alertname: SignerWiredNetworkLoss
          service: ${kubernetes_name_prefix}-tezos-remote-signer-forwarder-${baker}
        receiver: 'slack_${baker}'
        continue: true
%{ endif }
%{ if contains(keys(baking_node[baker]), "monitoring_email") }
      - match:
          alertname: NoRemoteSigner
          service: ${kubernetes_name_prefix}-tezos-remote-signer-loadbalancer-${baker}
        receiver: 'email_${baker}'
      - match:
          alertname: SignerPowerLoss
          service: ${kubernetes_name_prefix}-tezos-remote-signer-forwarder-${baker}
        receiver: 'email_${baker}'
      - match:
          alertname: SignerWiredNetworkLoss
          service: ${kubernetes_name_prefix}-tezos-remote-signer-forwarder-${baker}
        receiver: 'email_${baker}'
%{ endif }
%{   endfor }
%{ endfor }
      - receiver: 'null'
    receivers:
    - name: 'null'
    - name: 'slack_master'
      slack_configs:
      - channel: "#infra"
        send_resolved: true
        icon_url: https://avatars3.githubusercontent.com/u/3380462
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
        text: >-
          {{ range .Alerts -}}
          *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
    
          *Description:* {{ .Annotations.description }}
    
          *Details:*
            {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
            {{ end }}
          {{ end }}
%{ for baking_node in baking_nodes }
%{     for baker in keys(baking_node)}
%{ if contains(keys(baking_node[baker]), "monitoring_slack_url") }
    - name: 'slack_${baker}'
      slack_configs:
      - api_url: '${ baking_node[baker]["monitoring_slack_url"] }'
        channel: '${ baking_node[baker]["monitoring_slack_channel"] }'
        send_resolved: true
        icon_url: https://avatars3.githubusercontent.com/u/3380462
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
        text: >-
          {{ range .Alerts -}}
          *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
    
          *Description:* {{ .Annotations.description }}
    
          *Details:*
            {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
            {{ end }}
          {{ end }}
%{ endif }
%{ if contains(keys(baking_node[baker]), "monitoring_email") }
    - name: 'email_${baker}'
      email_configs:
      - to: '${ baking_node[baker]["monitoring_email"] }'
        send_resolved: true
        headers:
          subject: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
        html: ""
        text: >-
          {{ if eq .Status "firing" }}
          Your attention is required regarding the following Tezos Remote Signer alert:
          {{ else }}
          The following Tezos Remote Signer Alert is resolved:
          {{ end }}
          {{ range .Alerts -}}
          {{ .Annotations.summary }}
          {{ end }}
%{ endif }
%{ endfor }
%{ endfor }
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: prometheus-xtz
    release: mon-xtz
  name: tezos-remote-signer-loadbalancer-monitoring
spec:
  endpoints:
  - interval: 15s
    port: metrics
    path: /metrics
  selector:
    matchLabels:
      app.type: tezos-remote-signer-loadbalancer
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: prometheus-xtz
    release: mon-xtz
    prometheus: prometheus-xtz
    role: alert-rules
  name: tezos-remote-signer-loadbalancer-rules
spec:
  groups:
  - name: tezos-remote-signer.rules
    rules:
    - alert: NoRemoteSigner
      annotations:
        description: 'Remote signer is down'
        summary: Remote signer is down or unable to sign.
      expr: haproxy_backend_active_servers == 0
      for: 1m
      labels:
        severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: prometheus-xtz
    release: mon-xtz
    prometheus: prometheus-xtz
    role: alert-rules
  name: tezos-remote-signer-rules
spec:
  groups:
  - name: tezos-remote-signer.rules
    rules:
    - alert: SignerPowerLoss
      annotations:
        description: 'Remote signer has lost power'
        summary: Tezos remote signer has lost power
      expr: power != 0
      for: 1m
      labels:
        severity: critical
    - alert: SignerWiredNetworkLoss
      annotations:
        description: 'Remote signer has lost wired internet connection'
        summary: Tezos remote signer has lost wired internet connection
      expr: wired_network != 0
      for: 1m
      labels:
        severity: critical
    - alert: SignerWirelessNetworkLoss
      annotations:
        description: 'Remote signer has lost wireless internet connection'
        summary: Tezos remote signer has lost wireless internet connection
      expr: wireless_network != 0
      for: 1m
      labels:
        severity: critical
